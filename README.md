# Unstructured Data Analysis - Final Assignment
This repository stores the code and data for the UDA Final Assignment Project, alongside detailed instruction on how to run the code and reproduce the results in the report. This repository has been created by CID 01915547 on January 3rd, 2024.

## Repository Structure
This repository contains the following files and folders:
- `text_data/`: contains the company-level data used in the analysis. The data is stored in the following format: `text_data/{CIK}.json`. Every file contains: the CIK of the company, the Item 1 of the company's 10-K report, corresponding to the Business section of the report, and the time it took to download and parse the report. This folder is generated by the `build_raw_text_data.ipynb` notebook, see below for more details.
- `mappings.csv`: contains the cik, ticker, name_sec, name_sp500, added, sector, sub_industry, url fileds for every company in the S&P 500 index. This file is generated by the `build_mappings.ipynb` notebook, see below for more details.
- `build_raw_text_data.ipynb`: notebook that downloads the 10-K reports for the companies in the S&P 500 index and extracts the Business section of the report. The notebook stores the data in the `text_data/` folder.
- `build_mappings.ipynb`: notebook that downloads the list of companies in the S&P 500 index and extracts the relevant information for every company, especially its CIK and GICS code. The notebook stores the data in the `mappings.csv` file.
- `build_clusters.ipynb`: notebook that consumes the data in the `text_data/` folder and in the `mappings.csv` file and produces the clusters of companies based on the similarity of their Business section. This notebook contains all the results presented in the report.

## How to run the code
To reproduce all the results presented in the report, only the `build_clusters.ipynb` notebook needs to be run, provided that the `text_data/` folder and the `mappings.csv` file are present in the same working directory. The notebook specifies the Python version used and the required packages. The notebook can be run in any environment that satisfies these requirements. The notebook takes about 2 minutes to run, and uses parallel processing to speed up the computation in the Hyperparameter Tuning section (the number of parallel jobs is set to be the minimum between 5 and the number of available cores minus one). All the results presented in the report are also shown in the notebook.

Additionally, the `build_raw_text_data.ipynb` and `build_mappings.ipynb` notebooks can be run to reproduce the data used in the analysis. The notebooks specify the Python version used and the required packages. The notebooks can be run in any environment that satisfies these requirements. The author does not recommend running these notebooks, as they take a long time to run (about 2 hour for the `build_raw_text_data.ipynb` notebook, dependent on a good adn stable internet connection, and about 10 minutes for the `build_mappings.ipynb` notebook). The data generated by these notebooks is already present in the repository, so running these notebooks is not necessary to reproduce the results presented in the report. The notebooks are provided for completeness and transparency, and to show the substantial work that has been done to generate the data used in the analysis.

All the notebooks have been run on a MacBook Pro equipped with an M1 chip and 16GB of RAM. The author does not guarantee that the notebooks will run on other machines, but the author does not expect any issues to arise.

## Final Remarks for the Grader
The final project has been, in the author's opinion, interesting and challenging, providing a great opportunity to apply the concepts learned in the course to a real-world problem. It is the author's opinion that too often in academia it is 'assumed' that data is readily available, and that the data collection and cleaning process is not discussed in detail. Also, it is not often required to 'come up with' a problem to solve, as the problem is usually given. This project has been an opportunity to experience the whole process of data collection, cleaning, and analysis, and it is not surprising that this process has been the most time-consuming part of the project.

## Contact
The author is available for any questions or clarifications regarding the project.