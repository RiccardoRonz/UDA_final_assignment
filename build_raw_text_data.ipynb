{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "import unicodedata\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import lxml\n",
    "import cchardet\n",
    "import stopit\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]\n",
      "pandas version:  1.5.0\n",
      "requests version:  2.28.1\n",
      "bs4 version:  4.11.1\n",
      "lxml version:  4.9.3\n",
      "cchardet version:  2.1.7\n",
      "stopit version:  1.1.2\n"
     ]
    }
   ],
   "source": [
    "print('python version: ', os.sys.version)\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('requests version: ', requests.__version__)\n",
    "print('bs4 version: ', bs.__version__)\n",
    "print('lxml version: ', lxml.__version__)\n",
    "print('cchardet version: ', cchardet.__version__)\n",
    "print('stopit version: ', stopit.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "Helper functions are defined below. These functions are used to scrape the data from the SEC and Wikipedia. Each function is carefully documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str, \n",
    "             headers: dict = {'User-Agent': UserAgent().chrome}, \n",
    "             max_timeout: int = 60) -> bs.BeautifulSoup:\n",
    "    \"\"\"Get the soup object from a URL\n",
    "\n",
    "    Args:\n",
    "        url (str): URL.\n",
    "        headers (dict): Headers to be used in the request. Defaults to Chrome user agent.\n",
    "        max_timeout (int, optional): Maximum timeout in seconds. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        bs.BeautifulSoup: Soup object.\n",
    "    \"\"\"\n",
    "    # Get the response\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Transform the response into a soup object\n",
    "    with stopit.ThreadingTimeout(max_timeout) as timeout_ctx:\n",
    "        soup = bs.BeautifulSoup(response.text, 'lxml')\n",
    "    # If the parsing is successful, return the soup object\n",
    "    if timeout_ctx.state == timeout_ctx.EXECUTED:\n",
    "        return soup\n",
    "    # Otherwise, try by using a strainer to parse only the div tags\n",
    "    with stopit.ThreadingTimeout(max_timeout) as timeout_ctx:\n",
    "        soup_strainer = bs.SoupStrainer('div')\n",
    "        soup = bs.BeautifulSoup(response.text, 'lxml', parse_only=soup_strainer)\n",
    "    # If the parsing is successful, return the soup object\n",
    "    if timeout_ctx.state == timeout_ctx.EXECUTED:\n",
    "        return soup\n",
    "    # Otherwise, try by using a strainer to parse only the p tags\n",
    "    with stopit.ThreadingTimeout(max_timeout) as timeout_ctx:\n",
    "        soup_strainer = bs.SoupStrainer('p')\n",
    "        soup = bs.BeautifulSoup(response.text, 'lxml', parse_only=soup_strainer)\n",
    "    # If the parsing is successful, return the soup object\n",
    "    if timeout_ctx.state == timeout_ctx.EXECUTED:\n",
    "        return soup\n",
    "    # If the parsing is not successful, return None\n",
    "    return None\n",
    "\n",
    "def find_tags_with_text(soup: bs.BeautifulSoup,\n",
    "                        text_start: str = '', \n",
    "                        text_end: str = '', \n",
    "                        types: list = ['div', 'p']) -> list:\n",
    "    \"\"\"Find all tags of a given type that start with a given text and end with another given text\n",
    "\n",
    "    Args:\n",
    "        soup (bs.BeautifulSoup): Soup object.\n",
    "        text_start (str): Starting text of the tag. Defaults to ''.\n",
    "        text_end (str): Ending text of the tag. Defaults to ''.\n",
    "        types (list): List of types of tag to be searched for. Defaults to ['div', 'p'].\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags that match the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize containers for candidate and matching tags\n",
    "    candidate_tags = []\n",
    "    matching_tags = []\n",
    "    \n",
    "    # Normalize text\n",
    "    text_start, text_end = text_start.lower(), text_end.lower()\n",
    "    \n",
    "    # For each type of tag, find all tags that contain the starting or ending text\n",
    "    for type in types:\n",
    "        for tag in soup.find_all(type):\n",
    "            # Normalize text\n",
    "            tag_text = unicodedata.normalize('NFKD', tag.text.lower())\n",
    "            if (text_end in tag_text) or (text_start in tag_text):\n",
    "                candidate_tags.append(tag)\n",
    "    \n",
    "    # For each candidate tag, check if the tag itself or the tag and the next tag contain the starting and ending text\n",
    "    for i, tag in enumerate(candidate_tags):\n",
    "        # Normalize text\n",
    "        tag_text = unicodedata.normalize('NFKD', tag.text.lower()).strip()\n",
    "        # Get the text of the next tag if it exists\n",
    "        if i<(len(candidate_tags)-1):\n",
    "            next_tag = candidate_tags[i+1]\n",
    "            next_tag_text = unicodedata.normalize('NFKD', next_tag.text.lower()).strip()\n",
    "        else:\n",
    "            next_tag_text = ''\n",
    "        # Check matching criteria\n",
    "        if tag_text.startswith(text_start) and tag_text.endswith(text_end):\n",
    "            matching_tags.append(tag)\n",
    "        elif tag_text.startswith(text_start) and tag_text[:-1].endswith(text_end):\n",
    "            matching_tags.append(tag)\n",
    "        elif tag_text.startswith(text_start) and next_tag_text.startswith(text_end):\n",
    "            matching_tags.append(tag)\n",
    "            \n",
    "    return matching_tags\n",
    "\n",
    "def get_text_between_tags(soup: bs.BeautifulSoup,\n",
    "                          start_tag: bs.element.Tag, \n",
    "                          end_tag: bs.element.Tag,\n",
    "                          types: list = ['div', 'p']) -> str:\n",
    "    \"\"\"Get the text between two tags\n",
    "\n",
    "    Args:\n",
    "        soup (bs.BeautifulSoup): Soup object.\n",
    "        start_tag (bs.element.Tag): Starting tag.\n",
    "        end_tag (bs.element.Tag): Ending tag.\n",
    "\n",
    "    Returns:\n",
    "        str: Text between the two tags.\n",
    "    \"\"\"\n",
    "    # Initialize container for text\n",
    "    text = []\n",
    "    \n",
    "    # Get all tags\n",
    "    all_tags = soup.find_all(types)\n",
    "    \n",
    "    # Get the index of the starting and ending tags\n",
    "    start_tag_index = all_tags.index(start_tag)\n",
    "    end_tag_index = all_tags.index(end_tag)\n",
    "    \n",
    "    # If the starting tag is after the ending tag, return empty\n",
    "    if start_tag_index > end_tag_index:\n",
    "        return ''\n",
    "    \n",
    "    # Extract the text between the two tags\n",
    "    for tag in all_tags[start_tag_index:end_tag_index]:\n",
    "        text.append(tag.text)\n",
    "        \n",
    "    # Join the text\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_item1_from_soup(soup: bs.BeautifulSoup) -> str:\n",
    "    \"\"\"Get the item 1 text from a soup object\n",
    "\n",
    "    Args:\n",
    "        soup (bs.BeautifulSoup): Soup object.\n",
    "\n",
    "    Returns:\n",
    "        str: Item 1 text.\n",
    "    \"\"\"\n",
    "    # Specify diffrent combinations of starting and ending text\n",
    "    start_tags_start_text = ['item 1', 'part I', 'item']\n",
    "    start_tags_end_text = ['business']\n",
    "    end_tags_start_text = ['item 1a', 'part I', 'item']\n",
    "    end_tags_end_text = ['risk factors']\n",
    "    \n",
    "    # Find all combinations\n",
    "    combinations = list(itertools.product(\n",
    "        start_tags_start_text, start_tags_end_text, end_tags_start_text, end_tags_end_text))\n",
    "    \n",
    "    # Attempt to extract text. As soon as one combination works, return the text\n",
    "    for combination in combinations:\n",
    "        # Find all tags that match the criteria\n",
    "        start_tags = find_tags_with_text(soup, text_start=combination[0], text_end=combination[1])\n",
    "        end_tags = find_tags_with_text(soup, text_start=combination[2], text_end=combination[3])\n",
    "        # If there is at least one starting and one ending tag, extract the text between them\n",
    "        if len(start_tags)>0 and len(end_tags)>0:\n",
    "            text = get_text_between_tags(soup, start_tags[-1], end_tags[-1])\n",
    "            if len(text)>10:\n",
    "                return text\n",
    "    \n",
    "    # If no combination works, return empty\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape SEC Data\n",
    "\n",
    "The cell below scrapes the SEC data. The data is scraped from the SEC website and saved in several JSON files, one for each company. A detailed description of the process followed can be found in the accompanying report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting soup for 8670\n",
      "Getting soup for 860731\n",
      "Getting soup for 759944\n",
      "Getting item 1 for 759944\n",
      "Getting soup for 1022079\n",
      "Getting item 1 for 1022079\n",
      "Getting soup for 920522\n",
      "Getting item 1 for 920522\n",
      "Getting soup for 217346\n",
      "Getting item 1 for 217346\n",
      "Getting soup for 60086\n",
      "Getting item 1 for 60086\n",
      "Getting soup for 62996\n",
      "Getting item 1 for 62996\n",
      "Getting soup for 1601712\n",
      "Getting item 1 for 1601712\n",
      "Getting soup for 1679273\n",
      "Getting item 1 for 1679273\n",
      "Getting soup for 945841\n",
      "Getting item 1 for 945841\n",
      "Getting soup for 93556\n",
      "Getting item 1 for 93556\n",
      "Getting soup for 91440\n",
      "Getting item 1 for 91440\n",
      "Getting soup for 1336920\n",
      "Getting item 1 for 1336920\n",
      "Getting soup for 789570\n",
      "Getting item 1 for 789570\n",
      "Getting soup for 849399\n",
      "Getting item 1 for 849399\n",
      "Getting soup for 75677\n",
      "Getting item 1 for 75677\n",
      "Getting soup for 1324404\n",
      "Getting item 1 for 1324404\n",
      "Getting soup for 38777\n",
      "Getting item 1 for 38777\n",
      "Getting soup for 1145197\n",
      "Getting item 1 for 1145197\n",
      "Getting soup for 23217\n",
      "Getting item 1 for 23217\n",
      "Getting soup for 72331\n",
      "Getting item 1 for 72331\n",
      "Getting soup for 100517\n",
      "Getting item 1 for 100517\n",
      "Getting soup for 101778\n",
      "Getting item 1 for 101778\n",
      "Getting soup for 879169\n",
      "Getting item 1 for 879169\n",
      "Getting soup for 877212\n",
      "Getting item 1 for 877212\n",
      "Getting soup for 1748790\n",
      "Getting item 1 for 1748790\n",
      "Getting soup for 1286681\n",
      "Getting item 1 for 1286681\n",
      "Getting soup for 879101\n",
      "Getting item 1 for 879101\n",
      "Getting soup for 1754301\n",
      "Getting item 1 for 1754301\n",
      "Getting soup for 24545\n",
      "Getting item 1 for 24545\n",
      "Getting soup for 1070750\n",
      "Getting item 1 for 1070750\n",
      "Getting soup for 16732\n",
      "Getting item 1 for 16732\n",
      "Getting soup for 91576\n",
      "Getting item 1 for 91576\n",
      "Getting soup for 352541\n",
      "Getting item 1 for 352541\n",
      "Getting soup for 91419\n",
      "Getting item 1 for 91419\n",
      "Getting soup for 1564708\n",
      "Getting item 1 for 1564708\n",
      "Getting soup for 51434\n",
      "Getting item 1 for 51434\n",
      "Getting soup for 874761\n",
      "Getting item 1 for 874761\n",
      "Getting soup for 74208\n",
      "Getting item 1 for 74208\n",
      "Getting soup for 864749\n",
      "Getting item 1 for 864749\n",
      "Getting soup for 51644\n",
      "Getting item 1 for 51644\n",
      "Getting soup for 1065696\n",
      "Getting item 1 for 1065696\n",
      "Getting soup for 31791\n",
      "Getting item 1 for 31791\n",
      "Getting soup for 910606\n",
      "Getting item 1 for 910606\n",
      "Getting soup for 1792044\n",
      "Getting item 1 for 1792044\n",
      "Getting soup for 779152\n",
      "Getting item 1 for 779152\n",
      "Getting soup for 842023\n",
      "Getting item 1 for 842023\n",
      "Getting soup for 1711269\n",
      "Getting item 1 for 1711269\n",
      "Getting soup for 1285785\n",
      "Getting item 1 for 1285785\n",
      "Getting soup for 1170010\n",
      "Getting item 1 for 1170010\n",
      "Getting soup for 96943\n",
      "Getting item 1 for 96943\n",
      "Getting soup for 1100682\n",
      "Getting item 1 for 1100682\n",
      "Getting soup for 77360\n",
      "Getting item 1 for 77360\n",
      "Getting soup for 91142\n",
      "Getting item 1 for 91142\n",
      "Getting soup for 1590955\n",
      "Getting item 1 for 1590955\n",
      "Getting soup for 320335\n",
      "Getting item 1 for 320335\n",
      "Getting soup for 1037540\n",
      "Getting item 1 for 1037540\n",
      "Getting soup for 1013871\n",
      "Getting item 1 for 1013871\n",
      "Getting soup for 1111711\n",
      "Getting item 1 for 1111711\n",
      "Getting soup for 1604778\n",
      "Getting item 1 for 1604778\n",
      "Getting soup for 1732845\n",
      "Getting item 1 for 1732845\n",
      "Getting soup for 1841666\n",
      "Getting item 1 for 1841666\n",
      "Getting soup for 765880\n",
      "Getting item 1 for 765880\n",
      "Getting soup for 1725057\n",
      "Getting item 1 for 1725057\n",
      "Getting soup for 1048695\n",
      "Getting item 1 for 1048695\n",
      "Getting soup for 906345\n",
      "Getting item 1 for 906345\n",
      "Getting soup for 915389\n",
      "Getting item 1 for 915389\n",
      "Getting soup for 1278021\n",
      "Getting item 1 for 1278021\n",
      "Getting soup for 1579241\n",
      "Getting item 1 for 1579241\n",
      "Getting soup for 1590895\n",
      "Getting item 1 for 1590895\n",
      "Getting soup for 1501585\n",
      "Getting item 1 for 1501585\n",
      "Getting soup for 352915\n",
      "Getting item 1 for 352915\n",
      "Getting soup for 813828\n",
      "Getting item 1 for 813828\n",
      "Getting soup for 1174922\n",
      "Getting item 1 for 1174922\n",
      "Getting soup for 1043277\n",
      "Getting item 1 for 1043277\n",
      "Getting soup for 927066\n",
      "Getting item 1 for 927066\n",
      "Getting soup for 1000228\n",
      "Getting item 1 for 1000228\n",
      "Getting soup for 1370637\n",
      "Getting item 1 for 1370637\n",
      "Getting soup for 6201\n",
      "Getting item 1 for 6201\n",
      "Getting soup for 1043604\n",
      "Getting item 1 for 1043604\n",
      "Getting soup for 891103\n",
      "Getting item 1 for 891103\n",
      "Getting soup for 701985\n",
      "Getting item 1 for 701985\n",
      "Getting soup for 315213\n",
      "Getting item 1 for 315213\n",
      "Getting soup for 12208\n",
      "Getting item 1 for 12208\n",
      "Getting soup for 1037038\n",
      "Getting item 1 for 1037038\n",
      "Getting soup for 1267238\n",
      "Getting item 1 for 1267238\n",
      "Getting soup for 1513761\n",
      "Getting item 1 for 1513761\n",
      "Getting soup for 764622\n",
      "Getting item 1 for 764622\n",
      "Getting soup for 34903\n",
      "Getting item 1 for 34903\n",
      "Getting soup for 1116132\n",
      "Getting item 1 for 1116132\n",
      "Getting soup for 908255\n",
      "Getting item 1 for 908255\n",
      "Getting soup for 914208\n",
      "Getting item 1 for 914208\n",
      "Getting soup for 1474735\n",
      "Getting item 1 for 1474735\n",
      "Getting soup for 103379\n",
      "Getting item 1 for 103379\n",
      "Getting soup for 37785\n",
      "Getting item 1 for 37785\n",
      "Getting soup for 1596783\n",
      "Getting item 1 for 1596783\n",
      "Getting soup for 28412\n",
      "Getting item 1 for 28412\n",
      "Getting soup for 818479\n",
      "Getting item 1 for 818479\n",
      "Getting soup for 46080\n",
      "Getting item 1 for 46080\n",
      "Getting soup for 851968\n",
      "Getting item 1 for 851968\n",
      "Getting soup for 106640\n",
      "Getting item 1 for 106640\n",
      "Getting soup for 109380\n",
      "Getting item 1 for 109380\n",
      "Getting soup for 912595\n",
      "Getting item 1 for 912595\n"
     ]
    }
   ],
   "source": [
    "# Load mappings\n",
    "mappings = pd.read_csv('mappings.csv')\n",
    "\n",
    "# Only consider companies with a valid 10-K URL\n",
    "mappings = mappings.dropna(subset=['url'])\n",
    "\n",
    "# Get the text from Item 1 of the 10-K documents\n",
    "os.makedirs('text_data', exist_ok=True)\n",
    "for cik, url in mappings[['cik', 'url']].values:\n",
    "    if os.path.exists(f\"text_data/{cik}.json\"):\n",
    "        continue\n",
    "    start = time.time()\n",
    "    print(f\"Getting soup for {cik}\")\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        continue\n",
    "    print(f\"Getting item 1 for {cik}\")\n",
    "    text = get_item1_from_soup(soup)\n",
    "    end = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start))\n",
    "    res = {'cik': cik, 'item_1A': text, 'time': end}\n",
    "    with open(f\"text_data/{cik}.json\", \"w\") as outfile:\n",
    "        json.dump(res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "univ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
